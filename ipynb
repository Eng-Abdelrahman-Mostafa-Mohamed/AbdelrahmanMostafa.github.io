{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eng-Abdelrahman-Mostafa-Mohamed/AbdelrahmanMostafa.github.io/blob/main/ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXIHrDzo7kVM"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-CHFNpRCkqtb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PExmY3DU7oC3"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T1tVUlKhktL1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9rzArEQcktOS"
      },
      "outputs": [],
      "source": [
        "#loading the data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEkcy3U47sYX"
      },
      "source": [
        "Subset Class 0 and Class 1 from Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fBF4IYqCktR6"
      },
      "outputs": [],
      "source": [
        "# subsetting for classes 0,1 only\n",
        "\n",
        "x_train = x_train[np.logical_or(y_train == 0, y_train == 1)]\n",
        "y_train = y_train[np.logical_or(y_train == 0, y_train == 1)]\n",
        "\n",
        "x_test = x_test[np.logical_or(y_test == 0, y_test == 1)]\n",
        "y_test = y_test[np.logical_or(y_test == 0, y_test == 1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8Z1yfcuktUW",
        "outputId": "70db4fff-3238-4bc3-d9c0-c6851cf81057"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1], dtype=uint8), array([0, 1], dtype=uint8))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "np.unique(y_train), np.unique(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XVtiU95UktXi"
      },
      "outputs": [],
      "source": [
        "x_train=x_train.reshape(len(x_train),-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "stySGzFoktaf"
      },
      "outputs": [],
      "source": [
        "x_test=x_test.reshape(len(x_test),-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNbtcy4tktd4",
        "outputId": "c17d310a-299c-4ccc-8bc7-fd488f4dc7ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12665"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "x_train.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPIAmd--7vXJ"
      },
      "source": [
        "Standardize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rmdtq2nmktgS"
      },
      "outputs": [],
      "source": [
        "x_train=(x_train-np.mean(x_train,axis=0))/(np.std(x_train,axis=0)+10e-16)\n",
        "x_test=(x_test-np.mean(x_test,axis=0))/(np.std(x_test,axis=0)+10e-16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NEA5_BH7yL3"
      },
      "source": [
        "Divide data into training and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "vlnE5ENDktii"
      },
      "outputs": [],
      "source": [
        "X_train=x_train[0:10132]\n",
        "Y_train=y_train[0:10132]\n",
        "X_validation=x_train[10132:10132+2533]\n",
        "Y_validation=y_train[10132:10132+2533]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "EJKdyq82ktlE"
      },
      "outputs": [],
      "source": [
        "epochs=100\n",
        "eta=0.1\n",
        "tolerance = 10e-8\n",
        "Lambda=[0.01,0.1]\n",
        "#batch_size=10\n",
        "batch_size=[100,200]\n",
        "m = X_train.shape[0]\n",
        "ACCURACY=[]\n",
        "Name=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ek-oFD70WA"
      },
      "source": [
        "Sigmoid Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xTkhpSv2ktnn"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vp9jDxW72Kl"
      },
      "source": [
        "Cost Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "kIo5CfLbrSul"
      },
      "outputs": [],
      "source": [
        "def cost_function(phiz, y_train):\n",
        "    return (y_train * np.log(phiz) - (1 - y_train) * np.log(1 - phiz)).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFV-HVEb8Gq4"
      },
      "source": [
        "Cost Function with L1 Regularization(lasso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CpwMSyJ1ktqV"
      },
      "outputs": [],
      "source": [
        "def cost_function_L1(phiz, Y_train,w,Lambda):\n",
        "    return ((Y_train * np.log(phiz) - (1 - Y_train) * np.log(1 - phiz)).mean())+0.5*Lambda*np.mean(np.abs(w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEpqVoRr74NG"
      },
      "source": [
        "Accuracy Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "nulL5gHbmrU2"
      },
      "outputs": [],
      "source": [
        "def accuracy(x,y,w,b):\n",
        "  z=np.dot(x,w.T)+b\n",
        "  y_pred =sigmoid(z)\n",
        "  for i in range(len(y_pred)):\n",
        "    if (y_pred[i]>0.5):\n",
        "     y_pred[i]=1\n",
        "    else:\n",
        "      y_pred[i]=0\n",
        "  accuracy=np.mean(np.equal(y_pred, y))*100\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q6ELAIz76An"
      },
      "source": [
        "Gradient Descent Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "dEvThuIvk8EH"
      },
      "outputs": [],
      "source": [
        "def Gradient_Descent(X_train,Y_train,w,b,epochs,eta,Lambda):\n",
        "    for i in range (epochs):\n",
        "          z=np.dot(X_train,w.T)+b\n",
        "          phiZ = sigmoid(z)\n",
        "          Cost[i] =cost_function_L1(phiZ, Y_train,w,Lambda)\n",
        "          w = w - (eta * (np.dot((phiZ - Y_train).T , X_train) /len(X_train)+Lambda/len(X_train)*np.sign(w)))\n",
        "          b = b - eta * np.mean(phiZ - Y_train)\n",
        "          if (Cost[i]<tolerance):\n",
        "            break;\n",
        "     \n",
        "    return w,b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi7VoTElk8Gx",
        "outputId": "82f9ccda-5c2f-4cd4-aa76-3e4983240ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy  for train with Lambda ( 0.01 ) is = 98.5491512041058\n",
            "Accuracy for validation with Lambda ( 0.01 ) is = 98.61823924200553\n",
            "---------------------------------------------------------------------\n",
            "Accuracy  for train with Lambda ( 0.1 ) is = 98.56889064350572\n",
            "Accuracy for validation with Lambda ( 0.1 ) is = 98.53928148440583\n",
            "---------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for lamb in range(len(Lambda)):\n",
        "  w=np.random.rand(784)\n",
        "  b=np.random.rand(1)\n",
        "  Cost=np.zeros(epochs)\n",
        "  w,b=Gradient_Descent(X_train,Y_train,w,b,epochs,eta,Lambda[lamb])\n",
        "  ACCuracy=accuracy(X_train,Y_train,w,b)\n",
        "  print('Accuracy  for train with Lambda (',Lambda[lamb],') is =',ACCuracy)\n",
        "  Accuracy=accuracy(X_validation,Y_validation,w,b)\n",
        "  print('Accuracy for validation with Lambda (',Lambda[lamb],') is =',Accuracy)\n",
        "  print('---------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc39xLaAk8JO",
        "outputId": "8454d471-c60d-4016-debd-4ce752f96a64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.53928148440583"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "ACCURACY.append(Accuracy)\n",
        "Name.append('Gradient_Descent')\n",
        "Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUkSiFUs79pQ"
      },
      "source": [
        "Mini_Batch_Gradient_Descent Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go6fpwJFwL8v",
        "outputId": "912c320e-9fdd-4cea-e4d5-1579ce1acb55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the  1 Batch\n",
            "Accuracy for Train is = 99.98026056060007\n",
            "Accuracy for Validation is = 99.68416896960126\n",
            "this is the  2 Batch\n",
            "Accuracy for Train is = 100.0\n",
            "Accuracy for Validation is = 99.56573233320174\n"
          ]
        }
      ],
      "source": [
        "def mini_batch_gradient_descent(X_train, Y_train,w,b,eta,batch_size):\n",
        "  m=X_train.shape[0]\n",
        "  n=m/batch_size\n",
        "  n=int(n)\n",
        "  for i in range(epochs):\n",
        "    for j in range(0,m,n):\n",
        "     X_batch=X_train[j:j+n]\n",
        "     Y_batch=Y_train[j:j+n]\n",
        "     z=np.dot(X_batch,w.T)+b\n",
        "     phiZ = sigmoid(z)\n",
        "     Cost[i] =cost_function(phiZ, Y_batch)\n",
        "     w = w - (eta * np.dot((phiZ - Y_batch).T , X_batch)) /len(X_batch)\n",
        "     b = b - eta * np.mean(phiZ - Y_batch)\n",
        "     if (Cost[i]<tolerance):\n",
        "      break;\n",
        "      \n",
        "  return w,b\n",
        "\n",
        " \n",
        "\n",
        "for i in range(len(batch_size)):\n",
        "  print('this is the ',i+1,'Batch')\n",
        "  w=np.random.rand(784)\n",
        "  b=np.random.rand(1)\n",
        "  Cost=np.zeros(epochs)\n",
        "  w,b=mini_batch_gradient_descent(X_train, Y_train,w,b,eta,batch_size[i])\n",
        "  ACCuracy=accuracy(X_train,Y_train,w,b)\n",
        "  print('Accuracy for Train is =',ACCuracy)   \n",
        "  Accuracy=accuracy(X_validation,Y_validation,w,b)\n",
        "  print('Accuracy for Validation is =',Accuracy)   \n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Ppoc0xktxN",
        "outputId": "abcdd76a-0c63-4042-a6e7-a5160119d6e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.56573233320174"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "Accuracy=accuracy(X_validation,Y_validation,w,b)\n",
        "ACCURACY.append(Accuracy)\n",
        "Name.append('Mini_Batch_Gradient_Descent')\n",
        "Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX3okrzw8ACf"
      },
      "source": [
        "RMS_PROP Optimizer Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD4ZGQHqkt05",
        "outputId": "80824f8f-ca6d-4eca-dc75-a98a56440dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the  1 Batch\n",
            "Accuracy for Train is = 99.90130280300039\n",
            "Accuracy for Validation is = 99.36833793920252\n",
            "this is the  2 Batch\n",
            "Accuracy for Train is = 99.63482037110147\n",
            "Accuracy for Validation is = 99.17094354520331\n"
          ]
        }
      ],
      "source": [
        "def rms_prop(X_train, Y_train, w, b, eta, beta, epsilon,batch_size):\n",
        "  vdw=np.zeros_like(w)\n",
        "  vdb=np.zeros_like(b)\n",
        "  m=X_train.shape[0]\n",
        "  n=m/batch_size\n",
        "  n=int(n)\n",
        "  for i in range(epochs):\n",
        "    for j in range(0,m,n):\n",
        "     X_batch=X_train[j:j+n]\n",
        "     Y_batch=Y_train[j:j+n]\n",
        "     z=np.dot(X_batch,w.T)+b\n",
        "     phiZ = sigmoid(z)\n",
        "     dW = (np.dot((phiZ - Y_batch).T , X_batch)) /len(X_batch)\n",
        "     db=np.mean(phiZ - Y_batch)\n",
        "     #let assume or initialize v_dw by 0 or np.zeros(len(w))\n",
        "     vdw = beta*vdw+(1 - beta) * np.power(dW, 2)\n",
        "     vdb=beta*vdb+(1 - beta) * np.power(db, 2)\n",
        "     w= w-(eta / np.sqrt(vdw + epsilon)) * dW\n",
        "     b= b-(eta / np.sqrt(vdb + epsilon)) * db\n",
        "  return w,b\n",
        "\n",
        "     \n",
        "for i in range(len(batch_size)):\n",
        "  print('this is the ',i+1,'Batch')\n",
        "  w=np.random.rand(784)\n",
        "  b=np.random.rand(1)\n",
        "  Cost=np.zeros(epochs)\n",
        "  beta=0.9\n",
        "  epsilon=10e-8\n",
        "  w,b=rms_prop(X_train, Y_train,w,b,eta,beta,epsilon,batch_size[i])\n",
        "  ACCuracy=accuracy(X_train,Y_train,w,b)\n",
        "  print('Accuracy for Train is =',ACCuracy)   \n",
        "  Accuracy=accuracy(X_validation,Y_validation,w,b)\n",
        "  print('Accuracy for Validation is =',Accuracy)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNIbUWtRkt34",
        "outputId": "361a4272-5c2a-4737-db3d-530b8b945b5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.17094354520331"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "Accuracy=accuracy(X_validation,Y_validation,w,b)\n",
        "ACCURACY.append(Accuracy)\n",
        "Name.append('RMSPROP Optimizer')\n",
        "Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnZLklgd8DIF"
      },
      "source": [
        "ADAM Oprimizer Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "HpQmZKWtP6sQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e9bba0-fbd8-4e5e-96d1-7614904ad7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the  1 Batch\n",
            "Accuracy for Train is = 99.8026056060008\n",
            "Accuracy for Validation is = 99.36833793920252\n",
            "this is the  2 Batch\n",
            "Accuracy for Train is = 99.59534149230161\n",
            "Accuracy for Validation is = 99.249901302803\n"
          ]
        }
      ],
      "source": [
        "def adam_optimizer(X_train, Y_train, w, b, eta,beta1, beta2,epsilon,t,batch_size):\n",
        "      m=X_train.shape[0]\n",
        "      n=m/batch_size\n",
        "      n=int(n)\n",
        "      v_dw_RMS = np.zeros_like(w)\n",
        "      v_db_RMS = np.zeros_like(b)\n",
        "      v_dw_momentum = np.zeros_like(w)\n",
        "      v_db_momentum = np.zeros_like(b)                                               \n",
        "      epsilon = epsilon\n",
        "      for i in range(epochs):\n",
        "          indices = np.random.permutation(X_train.shape[0])\n",
        "          x = X_train[indices]\n",
        "          y = Y_train[indices]\n",
        "          for j in range(0, m, n):\n",
        "              x_batch = x[j:j+n]\n",
        "              y_batch = y[j:j+n]\n",
        "              z = np.dot(x_batch, w) + b\n",
        "              y_pred = 1 / (1 + np.exp(-z))\n",
        "              dz = y_pred - y_batch\n",
        "              dw = np.dot(x_batch.T, dz) / batch_size\n",
        "              db = np.sum(dz) / batch_size\n",
        "              # RMSprop part of velocity update\n",
        "              v_dw_RMS = beta2 * v_dw_RMS + (1 - beta2) * (dw**2)\n",
        "              v_db_RMS = beta2 * v_db_RMS + (1 - beta2) * (db**2)\n",
        "              # momentum part of velocity update\n",
        "              v_dw_momentum = beta1 * v_dw_momentum + (1 - beta1) * dw\n",
        "              v_db_momentum = beta1 * v_db_momentum + (1 - beta1) * db\n",
        "              # combine to make adam weights and bias \n",
        "              w -= eta * (v_dw_momentum / (np.sqrt(v_dw_RMS) + epsilon))\n",
        "              b -= eta * (v_db_momentum / (np.sqrt(v_db_RMS) + epsilon))\n",
        "      return w, b\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(batch_size)):\n",
        "  w=np.random.rand(784)\n",
        "  b=np.random.rand(1)\n",
        "  Cost=np.zeros(epochs)\n",
        "  beta1=0.9\n",
        "  beta2=0.9\n",
        "  epsilon=10e-8\n",
        "  t=1\n",
        "  print('this is the ',i+1,'Batch')\n",
        "  w,b=adam_optimizer(X_train, Y_train, w,b, 0.01, beta1, beta2, epsilon, t,batch_size[i])\n",
        "  ACCuracy=accuracy(X_train,Y_train,w,b)\n",
        "  print('Accuracy for Train is =',ACCuracy)   \n",
        "  Accuracy=accuracy(X_validation,Y_validation,w,b)\n",
        "  print('Accuracy for Validation is =',Accuracy)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Dr676jf3kt-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2670d737-3477-4fb3-c3ec-5fb187666f7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.249901302803"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "Accuracy=accuracy(X_validation,Y_validation,w,b)\n",
        "ACCURACY.append(Accuracy)\n",
        "Name.append('adam Optimizer')\n",
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Y3vAFS1jkuCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8fc747-8994-4da7-ad0e-a4451ae02bf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[98.53928148440583, 99.56573233320174, 99.17094354520331, 99.249901302803]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "ACCURACY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Ol3SXAxJkuGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543d2d2f-3b73-45ac-852f-6cbefa87d542"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.33806146572104"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# Test with same data and look at the accuracy\n",
        "z=np.dot(x_test,w.T)+b\n",
        "y_pred =sigmoid(z)\n",
        "for i in range(len(y_pred)):\n",
        "  if (y_pred[i]>0.5):\n",
        "     y_pred[i]=1\n",
        "  else:\n",
        "     y_pred[i]=0\n",
        "  \n",
        "    \n",
        "accuracy=np.mean(np.equal(y_pred, y_test))*100\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "af6rJX1fkuJu"
      },
      "outputs": [],
      "source": [
        "name=pd.DataFrame(Name,columns=['Optimizer'])\n",
        "ACCuracy=pd.DataFrame(ACCURACY,columns=['Accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "hGtjhTflkuMQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "f1efee12-01f8-46c4-c878-d34d75f6fd9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     Optimizer   Accuracy\n",
              "0             Gradient_Descent  98.539281\n",
              "1  Mini_Batch_Gradient_Descent  99.565732\n",
              "2            RMSPROP Optimizer  99.170944\n",
              "3               adam Optimizer  99.249901"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35fa3e57-17e9-465d-a281-042217c3c13c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gradient_Descent</td>\n",
              "      <td>98.539281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mini_Batch_Gradient_Descent</td>\n",
              "      <td>99.565732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RMSPROP Optimizer</td>\n",
              "      <td>99.170944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>adam Optimizer</td>\n",
              "      <td>99.249901</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35fa3e57-17e9-465d-a281-042217c3c13c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-35fa3e57-17e9-465d-a281-042217c3c13c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-35fa3e57-17e9-465d-a281-042217c3c13c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "Table=pd.concat([name,ACCuracy],axis=1)\n",
        "Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5TU9m8o7CTV"
      },
      "source": [
        "Use L1 regularization with gradient descent optimizer: This involves adding an L1 penalty term to the loss function for your logistic regression model, which encourages sparsity in the model weights. You can train this regularized model using gradient descent optimization, with a learning rate and regularization strength hyperparameters.\n",
        "\n",
        "\n",
        "\n",
        "Use mini-batch gradient descent optimizer: This involves dividing your dataset into multiple batches, and updating the model weights using the gradients computed on each batch. You can train this model using a variety of batch sizes and learning rates, and evaluate its performance on the validation set.\n",
        "\n",
        "\n",
        "Use RMSProp optimizer and Adam optimizer: These are both adaptive optimization algorithms that adjust the learning rate based on the magnitude of the gradients and the history of the parameter updates. You can train your logistic regression model using either of these optimizers, and tune the hyperparameters to obtain the best performance on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpX6gFiY7FbV"
      },
      "source": [
        "L1 regularization with gradient descent optimizer: The L1 regularization can help to reduce overfitting and improve the generalization performance of the model. The choice of the regularization strength hyperparameter can affect the sparsity of the model weights and its performance on the validation set.\n",
        "\n",
        "Mini-batch gradient descent optimizer: The mini-batch gradient descent can speed up the optimization process and reduce the variance of the gradient estimates. The choice of the batch size hyperparameter can affect the convergence rate and stability of the optimization algorithm.\n",
        "\n",
        "RMSProp optimizer: The RMSProp optimizer can adaptively adjust the learning rate based on the magnitude of the gradients and the history of the parameter updates. It can converge faster than gradient descent and mini-batch gradient descent for certain types of problems.\n",
        "\n",
        "Adam optimizer: The Adam optimizer combines ideas from momentum and RMSProp and is considered a state-of-the-art optimization algorithm. It can converge faster than other optimization algorithms, particularly for large datasets and complex models. However, it may be computationally expensive due to the additional computation required for maintaining the moving averages and may not always converge to the optimal solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdTl1HOA7MXO"
      },
      "source": [
        "The results of the implementation may vary depending on the random seed, but generally, we can see that the accuracy on the validation set is highest for the Adam optimizer, followed by the RMS Prop optimizer. The mini-batch gradient descent optimizer also performs well, with similar accuracy to the RMS Prop optimizer. The  gradient descent optimizer performs the worst in terms of accuracy.\n",
        "\n",
        "In conclusion, we observed that the Adam optimizer performed the best in terms of accuracy on the validation set. The RMS Prop optimizer and mini-batch gradient descent optimizer also performed well, while the standard gradient descent optimizer performed the worst. The choice of lambda value also had a significant impact on the performance, with smaller lambda values generally resulting in better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "HRe-4cPTkuTs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "fClhB6l-kuWU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "uJkgjfP9kt7_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}